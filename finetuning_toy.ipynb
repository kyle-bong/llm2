{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88494a83-f221-4712-995a-fdad93ac4042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import random\n",
    "from random import sample\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8abffd82-e7ec-471f-9df8-53bc7cbcf780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1465952-6d4c-4055-b7d9-ba9b4405b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 1234\n",
    "np.random.seed(SEED_NUM)\n",
    "random.seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f61c58d-25e2-423d-b369-498eb56c53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-5.8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb308b7-66fe-4f80-8bbf-9c57ca0edd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:00<00:00, 14.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/KoAlpaca-Polyglot\",\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 low_cpu_mem_usage=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "080e01ac-95f3-4d59-9b8f-a00b4e6d8362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "2048\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(model.config.max_position_embeddings)\n",
    "print(model.config.max_length)\n",
    "model.config.max_length = 2048\n",
    "print(model.config.max_length)\n",
    "model.config.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6932038-c184-4cc4-8adb-730ae2d48b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df_train = pd.read_json('dataset/total_train_dataset.json')\n",
    "df_val = pd.read_json('dataset/total_val_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc91235-2d76-4058-8ec8-9936fc9b0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace=True)\n",
    "df_val.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da18b323-eb70-4ea7-bb33-709dad560382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>info.essay_id</th>\n",
       "      <th>info.essay_prompt</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>score.essay_scoreT_avg</th>\n",
       "      <th>student.student_grade</th>\n",
       "      <th>paragraph2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ESSAY_72480</td>\n",
       "      <td>여러분은 저작권에 대해 들어보셨나요? 저작권이란, 저작권을 가진 소유자가 자신의 ...</td>\n",
       "      <td>저는 카피레프트 운동을 실시하는 것에 대해 찬성하는 입장입니다. 카피레프트로 인해 ...</td>\n",
       "      <td>22.333334</td>\n",
       "      <td>중등_2학년</td>\n",
       "      <td>저는 카피레프트 운동을 실시하는 것에 대해 찬성하는 입장입니다.#@문장구분# 카피레...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ESSAY_52262</td>\n",
       "      <td>반려동물을 키우는 가정이 급속도로 늘어나고 있습니다. 그에 비례하여 반려동물 및 ...</td>\n",
       "      <td>나는 동물사육을 제한하는 것은 옳지 않다고 생각한다.\\n 분명히 제한 규제나 다른 ...</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>중등_3학년</td>\n",
       "      <td>나는 동물사육을 제한하는 것은 옳지 않다고 생각한다.#@문장구분#\\n 분명히 제한 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ESSAY_54605</td>\n",
       "      <td>반려동물을 키우는 가정이 급속도로 늘어나고 있습니다. 그에 비례하여 반려동물 및 ...</td>\n",
       "      <td>지금 반려동물에 의해 일어나는 피해사례가 많아지고 있어, 사육에 대한 제도를 강화하...</td>\n",
       "      <td>28.466667</td>\n",
       "      <td>중등_3학년</td>\n",
       "      <td>지금 반려동물에 의해 일어나는 피해사례가 많아지고 있어, 사육에 대한 제도를 강화하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ESSAY_51646</td>\n",
       "      <td>과학 발전을 통해 다양한 에너지들이 만들어지고 일상생활에 사용되고 있습니다. 대표...</td>\n",
       "      <td>저는 생활의 편리함을 뒤로하고 에너지 절약을 먼저 하자는 의견입니다 환경은 미래에 ...</td>\n",
       "      <td>28.111109</td>\n",
       "      <td>초등_6학년</td>\n",
       "      <td>저는 생활의 편리함을 뒤로하고 에너지 절약을 먼저 하자는 의견입니다 #@문장구분#환...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ESSAY_68296</td>\n",
       "      <td>반려동물을 키우는 가정이 급속도로 늘어나고 있습니다. 그에 비례하여 반려동물 및 ...</td>\n",
       "      <td>나는 동물을 키우는 것을 법을 반대하는 것이 좋겠다고 생각하고 있다. 왜냐하면 우리...</td>\n",
       "      <td>28.066666</td>\n",
       "      <td>중등_3학년</td>\n",
       "      <td>나는 동물을 키우는 것을 법을 반대하는 것이 좋겠다고 생각하고 있다. #@문장구분#...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index info.essay_id                                  info.essay_prompt  \\\n",
       "0      0   ESSAY_72480   여러분은 저작권에 대해 들어보셨나요? 저작권이란, 저작권을 가진 소유자가 자신의 ...   \n",
       "1      0   ESSAY_52262   반려동물을 키우는 가정이 급속도로 늘어나고 있습니다. 그에 비례하여 반려동물 및 ...   \n",
       "2      0   ESSAY_54605   반려동물을 키우는 가정이 급속도로 늘어나고 있습니다. 그에 비례하여 반려동물 및 ...   \n",
       "3      0   ESSAY_51646   과학 발전을 통해 다양한 에너지들이 만들어지고 일상생활에 사용되고 있습니다. 대표...   \n",
       "4      0   ESSAY_68296   반려동물을 키우는 가정이 급속도로 늘어나고 있습니다. 그에 비례하여 반려동물 및 ...   \n",
       "\n",
       "                                           paragraph  score.essay_scoreT_avg  \\\n",
       "0  저는 카피레프트 운동을 실시하는 것에 대해 찬성하는 입장입니다. 카피레프트로 인해 ...               22.333334   \n",
       "1  나는 동물사육을 제한하는 것은 옳지 않다고 생각한다.\\n 분명히 제한 규제나 다른 ...               28.600000   \n",
       "2  지금 반려동물에 의해 일어나는 피해사례가 많아지고 있어, 사육에 대한 제도를 강화하...               28.466667   \n",
       "3  저는 생활의 편리함을 뒤로하고 에너지 절약을 먼저 하자는 의견입니다 환경은 미래에 ...               28.111109   \n",
       "4  나는 동물을 키우는 것을 법을 반대하는 것이 좋겠다고 생각하고 있다. 왜냐하면 우리...               28.066666   \n",
       "\n",
       "  student.student_grade                                         paragraph2  \n",
       "0                중등_2학년  저는 카피레프트 운동을 실시하는 것에 대해 찬성하는 입장입니다.#@문장구분# 카피레...  \n",
       "1                중등_3학년  나는 동물사육을 제한하는 것은 옳지 않다고 생각한다.#@문장구분#\\n 분명히 제한 ...  \n",
       "2                중등_3학년  지금 반려동물에 의해 일어나는 피해사례가 많아지고 있어, 사육에 대한 제도를 강화하...  \n",
       "3                초등_6학년  저는 생활의 편리함을 뒤로하고 에너지 절약을 먼저 하자는 의견입니다 #@문장구분#환...  \n",
       "4                중등_3학년  나는 동물을 키우는 것을 법을 반대하는 것이 좋겠다고 생각하고 있다. #@문장구분#...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa3447c6-e307-401e-886f-125cd64f4e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39591/39591 [00:00<00:00, 512125.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# full dataset\n",
    "train_data = []\n",
    "\n",
    "for prompt, sent, label in tqdm(df_train[['info.essay_prompt', 'paragraph', 'score.essay_scoreT_avg']].values):\n",
    "    # tokens = tokenizer(prompt + '\\n' + sent).input_ids\n",
    "    train_data.append((prompt, sent, label))\n",
    "    \n",
    "train_fewshot = sample(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d020721d-ef97-464d-b9a2-312d154dcf50",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31/5906 [00:16<51:35,  1.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(prompt_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,  max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m token_ids, attn_mask \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mcuda(), tokens\u001b[38;5;241m.\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 40\u001b[0m gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m pred \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(gen_tokens[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:1522\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1517\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:2339\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2339\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2347\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:673\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    671\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 673\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_neox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    687\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:564\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    556\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    557\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    558\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m         head_mask[i],\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 564\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:331\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     hidden_states: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    330\u001b[0m ):\n\u001b[0;32m--> 331\u001b[0m     attention_layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:149\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_layer_past:\n\u001b[1;32m    148\u001b[0m     seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m layer_past[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 149\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m query, key \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n\u001b[1;32m    151\u001b[0m query \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((query, query_pass), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:278\u001b[0m, in \u001b[0;36mRotaryEmbedding.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcos_cached \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mcos()[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msin_cached \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39msin()[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos_cached\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msin_cached[:seq_len, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "\n",
    "def build_prompt_text(sent):\n",
    "    return \"\\n\\n### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\\n\" + sent + \"\\n점수: \"\n",
    "\n",
    "def clean_text(sent):\n",
    "    # sent_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", sent)\n",
    "    sent_clean = sent\n",
    "    return sent_clean\n",
    "\n",
    "real_labels = []\n",
    "pred_tokens = []\n",
    "\n",
    "total_len = len(df_val[['info.essay_prompt', 'paragraph', 'score.essay_scoreT_avg']].values)\n",
    "for i, (prompt, sent, label) in tqdm(enumerate(df_val[['info.essay_prompt', 'paragraph', 'score.essay_scoreT_avg']].values), total=total_len):\n",
    "    prompt_text = ''\n",
    "    \n",
    "    for j in range(len(train_fewshot)):\n",
    "        ex_prompt, ex_text, ex_label = train_fewshot[j]\n",
    "        # cleaned_ex_text = clean_text('질문: ' + ex_prompt + '\\n' + '답안: ' +  ex_text)\n",
    "        cleaned_ex_text = clean_text('답안: ' +  ex_text)\n",
    "        appended_prompt_ex_text = build_prompt_text(cleaned_ex_text)\n",
    "        appended_prompt_ex_text += str(ex_label)\n",
    "        prompt_text += appended_prompt_ex_text\n",
    "    \n",
    "    # ex_prompt, ex_text, ex_label = train_data[i]\n",
    "    # cleaned_ex_text = clean_text(ex_prompt + '\\n' + ex_text)\n",
    "    # appended_prompt_ex_text = build_prompt_text(cleaned_ex_text)\n",
    "    # appended_prompt_ex_text += '\\n' + ex_label\n",
    "    # promp_text += appended_prompt_ex_text\n",
    "    \n",
    "    # cleaned_sent = clean_text('질문: ' + prompt + '\\n' + '답안: ' + sent)\n",
    "    cleaned_sent = clean_text('답안: ' + sent)\n",
    "    appended_prompt_sent = build_prompt_text(cleaned_sent)\n",
    "    \n",
    "    prompt_text += appended_prompt_sent\n",
    "    \n",
    "    tokens = tokenizer(prompt_text, return_tensors='pt',  max_length=2048, truncation=True, padding=True)\n",
    "    token_ids, attn_mask = tokens.input_ids.cuda(), tokens.attention_mask.cuda()\n",
    "    gen_tokens = model.generate(input_ids=token_ids, attention_mask=attn_mask,\n",
    "                                max_new_tokens=1, pad_token_id=0)\n",
    "    pred = tokenizer.batch_decode(gen_tokens[:, -1])[0].strip()\n",
    "    \n",
    "    try:\n",
    "        pred_tokens.append(float(pred))\n",
    "    except ValueError:\n",
    "        pred_tokens.append(pred)\n",
    "    real_labels.append(float(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9278942f-fdb6-4247-9fe9-68b1aec9529a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 위에 나온 가사의 곡은 중년의 가수 '김연자'님께서 만든 노래입니다. 중년의 가수분도 결혼은 선택이라고 생각하시는 모습을 보면 이미 대한민국의 사람들 대부분에게는 결혼은 선택이라는 생각을 가지고 있다고 볼 수가 있습니다. 사실 예전에는 결혼이 거의 필수라고 생각되었고, 출산 또한 4~5명씩 낳았을만큼 대를 이어가는 것에 대해서 큰 의미를 두었지만, 세상은 변했다고 생각합니다. 어떤 길도 좋고 나쁨을 단정지을수는 없고, 어떤 길을 선택할 것인지도, 개인의 자유입니다. 이만큼 개인의 자유를 보장해주는 것이 중요해진 사회에서 결혼을 필수라고 말하는 사람은 아마 고지식한 사람으로 생각될 수 있습니다. 또한 세상에는 여러 생각을 가진 사람이 있고, 그중에는 비혼주의를 가진 사람도 있습니다. 물론 저는 비혼주의가 아니라서 왜 그런 생각을 가지고 있는지는 알 수 없지만, 아마 자신의 삶을 결혼해서의 삶보다 더 중요시 여기기 때문이라고 생각합니다. 확실히 결혼을 하고 아이를 낳게 되면 OO엄마, OO아빠로 불리는 경우가 많습니다. 자신의 이름으로 자신의 삶을 살아가고 싶은 사람들이 비혼을 하거나 또는 결혼을 하더라도 출산을 하지 않으려고 하는 것이라고 생각합니다. 또한 아이를 낳아 키우려면 경제적인 비용도 많이 들기 때문에 출산까지는 많은 사람들이 하지 않으려고 합니다. 아이를 낳아 키우고 싶지만 경제적인 문제 때문에 꺼려하는 사람들도 있다고 생각합니다. 이런 여러 처지를 가진 사람들이 있기 때문에 결혼을 필수라고 말하기는 힘들 것이라고 생각합니다. 힘든 일을 억지로 하는 것은 그 사람에게 부담을 줄 수 있는 행위이며 온전히 본인의 삶이기 때문에 누구도 참견할 자격은 없습니다.\n",
      "점수: 28.666666\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 다문화 가정은 아버지 혹은 어머니가 외국 국적일 경우를 말합니다.    \n",
      "국가 정책을 통해 다문화 가족에 대해 이해 교육을 펼쳐 국민들의 생각을  변화시키려 했지만, 여전히 사회나 학교 등에서 차별과 편견이 남아있는 것 같습니다. 생김새와 문화가 한국인들과는 다르다는 이유로 다문화 가정 아이들 이 사람들의 실은 시선에 상처받는 경우가 많습니다.다문화 가족의 아이들이 차별 혹은 편견 받지않고 우리와 동일한 한국인으로서 인정받고 존중하는 사회를 만듭시다\n",
      "점수: 26.261904\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 과학기술을 줄여야 한다고 생각합니다. 왜냐하면 과학기술이 좋긴 하지만 환경이 사라지면 우리의 식량을 잃고 동물들의 집도 일고 자원도 사라지면서 모든게 끝나기 때문입니다. 그래서 저는 에너지 사용을 줄이고 지구의 환경을 지키면서 다짐을 하겠습니다. 우선 첫째 쓰레기는 쓰래기통에 버리기로 하겠습니다. 다짐 둘째 애너지는 그때그때 잘 사용해서 절략을 줄인다. 다짐 셋째 환경에 파괴되는 것은 모두 줄이고 버리겠습니다. 다짐 넷째 분리수거를 잘 해야겠다. 왜냐하면 환경파괴에 문제는 쓰레기 때문에 분리수거를 잘 하기로 결심했다. 다짐 다섯째 사용한 것을 재자리로 같다 놓기 왜냐하면 어디에 두다가 나중에 쓰래기가 되어서 환경을 파괴 할수 있어서 재자리에 두는게 좋을것 같다. 나의 다짐을 끝까지 지켜서 환경을 지킬거다.\n",
      "점수: 21.566666\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 여러분, 여러분은 평일과 주말에 학습활동만 한다면 어떠실 것 갔나요? 저는 너무 너무 스트레스가 쌓일 것 갔은 데요. 그래서 필요한 활동이 여가활동 인 데요. 저는 학습활동 4시간 30분 여가활동 4시간 아님 3시간 30분정도 있습니다. 저는 학습활동만 하면 머리가 어질어질, 하기 싫어 집니다. 그래서 저는 학습활동을 하면서 고칠 점이 공부할 때 집중입니다. 여가활동의 고칠 점은 \"너무 여가활동에 너무 집중하지말자\" 입니다. 너무 공부에 집중하면 안되고 여가활동에도 너무 집중하면 안됩니다. 여가활동을 하는 이유는 학습활동을 하면서 복잡해진 뇌를 쉬게 해주는 것이기 때문에 여가활동이 필요한 것 입니다.\n",
      "점수: 26.860315\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 저는 모든 작품은 저작권으로써 보호 받을 권리가 있다고 생각합니다. 그래서 저는 사회적 자산을 기반으로 만들었다고 해서 저작권으로 보호해주지 않는 것은 그 작품을 만든 사람에게 있어서 도리가 아니라고 생각합니다. 작품을 만든 사람이 좋은 마음으로 만들었다고 하더라고 자신의 작품이 여기저기서 마음대로 돌아다니는 것을 보게 된다면 기분이 좋지 않을 수도 있다고 생각합니다. 왜냐하면 그 작품도 자신의 노력과 시간이 깃들어있는 작품일테니깐요. 그래서 저는 카피레프트 운동을 안 했으면 좋겠습니다. 다른 사람들의 입장에서는 좋은 취지의 운동일지 몰라도 작품을 만드는 입장에서는 기분 나쁠 수 있는 운동이라고 생각합니다.  그래서 저는 카피레프트 운동에 반대합니다. 저는 작품을 만드는 사람의 입장에서 먼저 생각을 해주어야한다고 생각하기 때문에 작품을 만드는 입장에서 생각을 해봤을 때는 카피레프트 운동은 안 하는 것이 맞는 것 같습니다. 이런 운동으로 인해 작품을 만드는 사람의 노력과 시간이 다른 사람들에게는 그냥 아무것도 아닌 작품이 되는 것 같다는 생각이 들었습니다. 만드는 사람의 노력과 시간을 인정해주는 것은 함부로 아무나 쓰지 못하게 막는 것이라고 생각합니다. 그래서 저는 카피레프트 운동을 반대합니다.\n",
      "점수: 23.833334\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 상어의 모험\n",
      " 나는 상어. 그냥 평범한 상어이다. 나는 한 곳 에 있는 것을 싫어해 모험을 하곤한다. \"오늘도 모험을 하러 가볼까?나는 배가 곱으면 물고기들을 먹지만 고래들은 조심해야 한다. \"어쨌든 출발!\"나는 바다 밑으로 깊숙이 들어갔다. 처음 보는 물고기들이 많아 신기했다. 맛있어 보였자만 처음보는 거라 겁나서 먹지 않았다. 나는 다시 위로 갔다. 위로가서 줄돔, 새우등으로 배를 체우고 다시 가기 시작했다. 가던중 배를 발견했다. 배 안에는 만은 금들이 있었다. 나는 필요 없어서 그냥 갔다. 더더 가다보는 먹이가 걸려있는 실을 봤다. 배가 고파서 물었더니, 누가 나를 당겼다. 인간이었다. 나는 놀라 인간들을 공격했다. 기절한 것을 보고 다시 바다로갔다. 이제 쉬려고 바위에 누우니 바위가 움직였다. 으악?거북이었다. 나는 미안 하다고 한뒤 집으로 돌아가야겠다고 생각해 집에 가려고했다. 그런데 문제가 생겼다.길을 잃어 버린 것이었다. 나는 아까 그 거북이에게 나의 집인 상집지역이 어디냐고 물어 보았다. 거북이는 자신도 길치라며 사라져 버렸다. 아무래도 오늘은 집에 돌아가기는 틀렸다. \"이렇게 긴 모험은 처음이야\"일주일뒤, 나는 아직도 집을 찾지 못했다. 나는 그냥 집을 만들어 이곳에 살기로 결심했다. 나는 나무 조각을 배개로 썼고 미역줄기들을 모아 이불로 썼다. 1년뒤, \"다시 모험을 시작해야지!\" 어? 여기는... 나는 내가 한심했다. 내집에 바로 뒤에 나의 옛날집이 있는데 못찾았다니, 옛날에 물어보지 말고 찾았어야 됬고 집 잃었다고 누워만 있으면 안됬다. 나는 다음생에는 열심히 살아야 겠다고 생각하고 일부로 낚시 줄에 걸려 자살을 했다. 죽기전 나는 항상 열심해야 겠다고 생각했다.\n",
      "점수: 22.116667\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 나는 e스포츠가 스포츠라고 인정을 받아도 된다고 생각한다. 그 이유는 e스포츠도 스포츠처럼 지금도 세계적으로 경기를 하고 있고 많은 사람들이 세계 나라별로 게임 랭킹이 어떻게 되는지 관심을 가지고 있는 사람들이 많이 있기 때문에 인기도 많이 있다고 할 수 있다. 그리고 e스포츠도 스포츠처럼 종목같은것을 정해서 그것을 경기때 게임을 하면 불연속성의 문제도 해결할 수 있고 리그오브레전드나 배틀그라운드처럼 예전에 만들어진 것이지만 지금까지 여전히 인기가 많이 있고 초등학생들부터 아저씨들까지 다양한 사람들이 즐길수 있는 것이기 때문에 그런 게임들은 유행을 타지 않는 게임이라고 생각해도 될 것 같다는 생각이 들었다. 그리고 사람들이 게임을 어렵다고 생각하는데 어떤 인터넷에서 어려운 것이 아니라 낯선 것이라고 말하는 것을 본 적이 있다. 나도 그 말이 맞는 말이라고 생각이 들었고 사람들이 게임을 낯설게 생각하고 안좋은 것이라고만 생각하기 때문에 게임이 어려운 것이라고 생각도 하는 것인 것 같다. 그래서 사람들이 스포츠 방송을 텔레비전에서 틀어주는 것처럼 게임방송도 그냥 공중파같은곳에서 틀어준다면 자주 보게 되니까 그만큼 낯선 느낌도 사라지게 될 것이고 e스포츠도 스포츠가 맞다고 인정해주는 사람들도 점점 늘어나게 될 것이다.\n",
      "점수: 27.314814\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 저는 다수의 의견이 좋습니다. 왜냐하면 의견이 하나로 안모아지면 많은 사람들이 불공평 하게 느낄수있고, 억울 할수도 있기 때문입니다. 예를 들면 A, B 팀이 있는데 A팀에는 12명이 의견을 내고 B팀에는 15명의 의견을 냈는데 서로 자기의견을 하자고 하면 안맞기 때문에 이상황에 다수결이 제일 괜찮다고 생각합니다. 저도 에전에 친구들이랑 오월드갈때 나눠서 갈껀지, 같이 다닐껀지 의견을 냈는데 서로 안맞고 기분도 서먹해서 친구들이 한사람 의견만 들어줘서 결국엔 오월드도 못가고, 분위기도 안 좋아지고, 점점 어색하기만 하고, 그래서 지금 생각해보니 후회가 되고, 내가 왜 그랬을까 궁금하기도 했습니다. 저는 다른 사람의 의견도 들어주고 싶지만, 다수결이 제일 괜찮다고 생각합니다.\n",
      " 예전에는 다수결이 없어서 많은 사람들이 힘들어 했습니다. 이러므로 제의견은 다수결 제일 좋다고 생각합니다.\n",
      "점수: 29.20635\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 나의 꿈은 의사가 되는 것입니다. 그래서 아프리카라든지 오지로 가서 의료시설이 부족한곳에 가서 그곳의 사람들을 도와주면서살고 싶습니다 지금 공부도 반에서 1등을 합니다.의사가 되기위해서는 일단 의대에 가야하기 때문에 앞으로 몇 년후에는 꼭의대에 붙어야합니다 그러기위해서는 지금보다는더많이 공부를 해야한다는걸 잘압니다.그래서시험기간이 아니라도 친구들과 노는시간보다는 책을 읽거나 공부하는데 많은 시간을 보냅니다 친구들은 저보고 왜 재미없는 공부만하냐고 하지만 10년20년후 나의 먼미래를 생각하면 이시간이 마냥 지겹지만은 않습니다.의사는 단지 공부만 잘해서 되는것이라는걸 잘압니다. 특히 내가 바라는 의사는 돈을 많이 버는것보다도 의료시설이 없는 사람들을 도와주는 것이기 때문에 봉사라는 정신이 더많아야하고 인간에대한 기본적인 사랑이 넘쳐야함을 잘압니다 그래서 지금도 아버지를 따라 양로원 봉사활동에 갑니다 거기서 조금 거부감이 들때도 있지만 할아버지 할머니들에게진심을 다해서 웃어들이고 놀아들이고 이야기를 들어주고 합니다\n",
      "점수: 23.944443\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 나는 부모의 능력과 경제력은 자본주의 사회에서 합법적으로 얻은 것이므로 사회의 불평등을 만든다는 것은 잘못된 표현이라고 생각한다. 그 이유는 오늘날 사람들은 자본주의 경제를 일상생활로 받아드리며 자신의 이익을 위해 사회나 국가의 간섭 없이 스스로 살아가고 있기 때문이다. 즉, 자신의 판단으로 열심히 일을 하거나 장사를 하여 돈을 벌거나, 물건을 소비하는 것은 당연한 것이다. 그러므로 부모는 자신의 재산을 자녀의 교육에 쓰는 것은 법을 어기는 것이 아니다. 훗날 자녀가 더 좋은 환경에서 살게 하는 것은 부모라면 누구라도 원할 것이다. 이러한 부모의 활동은 결국 자연스러운 경쟁으로 통해 사회의 발전을 가져오게 한다. 돈을 버는 것은 자본주의에서 개인의 자유와 권리에 해당된다. 그렇게 모은 재산은 사적인 소유권을 가진다. 이를 자녀의 교육비로 자유롭게 처분할 권리가 있다. 물론, 부모의 능력과 경제력이 사교육의 격차를 벌려 사회의 불평등을 가져올 수 있다고 말할 수도 있다. 하지만 부모라면 당연히 자녀에 대한 교육에 관심을 가져야하고, 더 좋은 성과를 얻기 위해 사교육에 재산을 사용하는 것은 바람직한 현상이다. 따라서 나는 부모의 능력과 경제력은 자본주의 사회에서 자연스러운 흐름이므로 사회의 불평등보다는 오히려 이러한 경쟁을 통해 사회의 발전을 가져다준다고 생각한다.\n",
      "점수: 28.211111\n",
      "\n",
      "##### 다음 답안의 수준을 0점부터 30점까지의 점수로 평가해주세요.\n",
      "답안: 저는 남한과 북한의 통일에 대해 찬성합니다. 왜냐하면 원래부터 남한과 북한은 하나의 국가였기 때문에 다시 한민족으로 통일되어야 합니다. 통일이 되는 그날까지 남한과 북한은 서로의 상대 국가에 대한 이해와 교류를 통해 경제, 사회, 문화의 차이를 좁히고 수준을 맞춰가며 준비 하여야 합니다. 만약에 남한과 북한이 통일된다면 관광, 국방, 자원 등 세계적으로 막강하고 부러운 최고의 국가가 될 것입니다. 따라서 저는 남한과 북한은 반드시 통일이 되어야 한다고 생각합니다.\n",
      "점수: \n"
     ]
    }
   ],
   "source": [
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2960f8c0-9b62-4b1e-846e-532495089d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PearsonRResult(statistic=nan, pvalue=nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "from scipy.stats import pearsonr\n",
    "score = pearsonr(pred_tokens, real_labels)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37ba4b-e89d-4565-97a4-50dd432b01a0",
   "metadata": {},
   "source": [
    "# Full Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b3e11e-cad2-4b50-a5a9-016ae7dc25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09abb23-90f1-4b0d-8161-1f545a80136a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262a4a47-1e0b-4bc4-a70f-92f9201fcd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### 질문: 양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\\n\\n##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### 질문: 스웨터의 유래는 어디에서 시작되었나요?\\n\\n### 답변: 스웨터의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### 질문: 토성의 고리가 빛의 띠로 보이는 이유는 무엇인가요?  \\n\\n토성의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### 질문: 화장품 OEM과 화장품 ODM의 차이점은 무엇인가요?\\n화장품 자체 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### 질문: '사이보그'는 언제 처음 등장한 말이며, 그 의미와 종류에는 어떤 것...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  ### 질문: 양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\\n\\n##...\n",
       "1  ### 질문: 스웨터의 유래는 어디에서 시작되었나요?\\n\\n### 답변: 스웨터의 ...\n",
       "2  ### 질문: 토성의 고리가 빛의 띠로 보이는 이유는 무엇인가요?  \\n\\n토성의 ...\n",
       "3  ### 질문: 화장품 OEM과 화장품 ODM의 차이점은 무엇인가요?\\n화장품 자체 ...\n",
       "4  ### 질문: '사이보그'는 언제 처음 등장한 말이며, 그 의미와 종류에는 어떤 것..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('train_v1.1b/KoAlpaca_v1.1a_textonly.json', lines=True)\n",
    "df.head()\n",
    "# print(len(df))\n",
    "# print(df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e411fff-ba8f-478b-873a-70013138c589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906f499e-ce80-41a4-99d4-53d7655cfcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 질문: 양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\\n\\n### 답변: 양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.<|endoftext|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7680749-c4e1-414a-bdbd-b02b7043d165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문: 양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\n",
      "\n",
      "### 답변: 양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \n",
      "\n",
      "식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\n",
      "\n",
      " 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \n",
      "\n",
      "고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ccf74-6a4a-4491-92b0-b68ce978e83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQC67Y74y+iOqa54+F/HqDyOlGVdWnUlcs62znFMxJrREL57bu9ZnaEPflvXBSs09I4mSAbii49awsO1RdtZNjMJiLLMcObb9NWOqw2FHo0zCK7XYUd84tJ1qm73B8uLDGPpuo5laKCPEhl5bfqMG63Qb51vl1PlWDMcs4B7mXl7+w==')}\n",
      "  warn(msg)\n",
      "/opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('KoAlpaca/finetuning_toy.ipynb')}\n",
      "  warn(msg)\n",
      "/opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/opt/conda/envs/llm2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"beomi/polyglot-ko-12.8b-safetensors\"  # safetensors 컨버팅된 레포\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1316e-fb76-40c4-b067-3af8e49f401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d3a83-3df9-4f94-adfd-9789d6707517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c6c292-44c6-4e04-85e9-ec8319980856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import pandas as pd\n",
    "df_train = pd.read_json('dataset/total_train_dataset.json')\n",
    "df_val = pd.read_json('dataset/total_val_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49974879-ace4-4875-a022-c59b7d90483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a2a186f-59b0-4d63-93a3-60c00d2ff732",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      4\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[1;32m      5\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(\u001b[43mmodel\u001b[49m, config)\n\u001b[1;32m     13\u001b[0m print_trainable_parameters(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba3cd163-9ab8-4ca1-a279-58066f094fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(grade, essay_prompt, paragraph, score):\n",
    "    result = '### 학년: ' + grade + '\\n\\n'\n",
    "    result += '### 질문: ' + essay_prompt + '\\n\\n'\n",
    "    result += '### 답안: ' + paragraph + '\\n\\n'\n",
    "    result += '### 점수: ' + str(score)+ '<|endoftext|>'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82e6ee7a-d5fa-49d0-a271-366c5785d6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 학년: 고등3\\n\\n### 질문: 뭐냐?\\n\\n### 답안: 나다\\n\\n### 점수: 32<|endoftext|>'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prompt('고등3', '뭐냐?', '나다', '32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7ae151e0-76f6-493e-9ef0-4c72fd4d27cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39591/39591 [00:00<00:00, 127817.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "# full dataset\n",
    "train_dataset = []\n",
    "for grade, essay_prompt, paragraph, score in tqdm(df_train[['student.student_grade', 'info.essay_prompt', 'paragraph', 'score.essay_scoreT_avg']].values):\n",
    "    train_dataset.append({'text':make_prompt(grade, essay_prompt, paragraph, score)})\n",
    "\n",
    "# to huggingface dataset\n",
    "train_dataset = Dataset.from_dict(pd.DataFrame(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "74462b89-9bac-498c-b86e-20ec129f7d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "train_dataset = train_dataset.map(lambda samples: tokenizer(samples['text']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d53f9dd-abe9-48f9-a856-bc6574898c14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'str2optimizer8bit_blockwise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 22\u001b[0m\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      8\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:2007\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2005\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2007\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2008\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/accelerate/optimizer.py:134\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     scale_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mget_scale()\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    136\u001b[0m     scale_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mget_scale()\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     raise RuntimeError(\"step() has already been called since the last update().\")\n\u001b[1;32m    327\u001b[0m retval = None\n\u001b[1;32m    329\u001b[0m if (hasattr(optimizer, \"_step_supports_amp_scaling\") and optimizer._step_supports_amp_scaling):\n\u001b[1;32m    330\u001b[0m     # This optimizer has customized scale-handling logic, so we can call optimizer.step() directly.\n\u001b[1;32m    331\u001b[0m     # The contract with custom optimizers is that their step() should accept an additional,\n\u001b[1;32m    332\u001b[0m     # optional grad_scaler kwarg.  We append self to the kwargs so the custom optimizer has full information:\n\u001b[1;32m    333\u001b[0m     # it can query its own state, invoke unscale_ on itself, etc\n\u001b[1;32m    334\u001b[0m     # The contract above is being deprecated to avoid introducing `grad_scaler: GradScaler` argument\n\u001b[1;32m    335\u001b[0m     # to `Optimizer.step`. The new behavior is going to add two Tensor attributes of `grad_scale`\n\u001b[1;32m    336\u001b[0m     # and `found_inf` to the passed optimizer so that the optimizer can utilize those\n\u001b[1;32m    337\u001b[0m     # to skip the parameter updates or unscale gradients before updating parameters in\n\u001b[0;32m--> 338\u001b[0m     # the fused kernel, e.g. `FusedAdamMathFunctor`.\n\u001b[1;32m    339\u001b[0m     # In this behavior, `GradScaler._check_inf_per_device` is called if `OptState.READY`,\n\u001b[1;32m    340\u001b[0m     # while the method is expected to be called by users side, i.e. their optimizers.\n\u001b[1;32m    341\u001b[0m     kwargs_ = kwargs\n\u001b[1;32m    342\u001b[0m     has_grad_scaler_kwarg = \"grad_scaler\" in inspect.signature(optimizer.step).parameters\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:285\u001b[0m, in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    284\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unscale_grads_(optimizer, inv_scale, found_inf, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 285\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m instance_ref()\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m method\n\u001b[1;32m     64\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     66\u001b[0m     instance \u001b[38;5;241m=\u001b[39m instance_ref()\n\u001b[1;32m     67\u001b[0m     instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m _fused_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mfused (bool, optional): whether the fused implementation (CUDA only) is used.\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124m            Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124m            are supported. (default: None)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124m              we want to give it sufficient bake-in time, so we default to foreach and NOT\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124m              fused when the user has not specified either flag.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    101\u001b[0m _capturable_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mcapturable (bool, optional): whether this instance is safe to\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124m            capture in a CUDA graph. Passing True can impair ungraphed performance,\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124m            so if you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt intend to graph capture this instance, leave it False\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124m            (default: False)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    106\u001b[0m _differentiable_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mdifferentiable (bool, optional): whether autograd should\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124m            occur through the optimizer step in training. Otherwise, the step()\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124m            function runs in a torch.no_grad() context. Setting to True can impair\u001b[39m\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;124m            performance, so leave it False if you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt intend to run autograd\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124m            through this instance (default: False)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    112\u001b[0m _maximize_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mmaximize (bool, optional): maximize the params based on the\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124m            objective, instead of minimizing (default: False)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_optimizer_step_pre_hook\u001b[39m(hook: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RemovableHandle:\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/optim/optimizer.py:269\u001b[0m, in \u001b[0;36mOptimizer8bit.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_state(group, p, gindex, pindex)\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefetch_state(p)\n\u001b[0;32m--> 269\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_paged:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# all paged operation are asynchronous, we need\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# to sync to make sure all tensors are in the right state\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/optim/optimizer.py:517\u001b[0m, in \u001b[0;36mOptimizer2State.update_step\u001b[0;34m(self, group, p, gindex, pindex)\u001b[0m\n\u001b[1;32m    515\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax2\u001b[39m\u001b[38;5;124m\"\u001b[39m], state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_max2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_max2\u001b[39m\u001b[38;5;124m\"\u001b[39m], state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock_wise\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 517\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_update_8bit_blockwise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqmap1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqmap2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabsmax1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabsmax2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgnorm_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgnorm_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_zeros\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskip_zeros\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/functional.py:1213\u001b[0m, in \u001b[0;36moptimizer_update_8bit_blockwise\u001b[0;34m(optimizer_name, g, p, state1, state2, beta1, beta2, eps, step, lr, qmap1, qmap2, absmax1, absmax2, weight_decay, gnorm_scale, skip_zeros)\u001b[0m\n\u001b[1;32m   1211\u001b[0m is_on_gpu([g, p, state1, state2, qmap1, qmap2, absmax1, absmax2])\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m g\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32 \u001b[38;5;129;01mand\u001b[39;00m state1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[0;32m-> 1213\u001b[0m     optim_func \u001b[38;5;241m=\u001b[39m \u001b[43mstr2optimizer8bit_blockwise\u001b[49m[optimizer_name][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m g\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;129;01mand\u001b[39;00m state1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m   1215\u001b[0m     optim_func \u001b[38;5;241m=\u001b[39m str2optimizer8bit_blockwise[optimizer_name][\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'str2optimizer8bit_blockwise' is not defined"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=500,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17352d-267f-49ca-817c-5ab5a7c8dd31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2324b-144c-4edd-a0cc-21478bb60d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5febf133-2057-4b4c-a73d-e6a163950e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(**tokenizer(\"### 답안: 노트북이 꼭 맥북이 아니더라도 스타벅스에 들어갈 수 있어야 한다. 왜냐하면 누구든지 자신이 마음에 드는 노트북을 가지고 다닐 권리가 있기 때문이다.\", return_tensors='pt', return_token_type_ids=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b871de-1da3-4cc0-aa06-f723338532b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecf3fc-9758-47bf-94ca-b336c2cdcdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "llm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
